{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tftext\n",
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Chekcer\n",
    "\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        self.shapes = dict()\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "        \n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "        \n",
    "        if rank != len(names):\n",
    "            raise ValueError(f\"Rank mismatch:\\n\"\n",
    "                             f\"     found {rank}: {shape.numpy()}\\n\"\n",
    "                             f\"     expected {len(names)}: {names}\"\n",
    "                             )\n",
    "        \n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "            \n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"     found: {new_dim}\\n\"\n",
    "                                 f\"     expected: {old_dim}\\n\"\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    cache_subdir=pathlib.Path.cwd(),\n",
    "    cache_dir=pathlib.Path.cwd(),\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for target, inp in pairs]\n",
    "    target = [target for target, input in pairs]\n",
    "\n",
    "    return target, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "target, inp = load_data(path_to_file)\n",
    "print(inp[-1], '\\n', target[-1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 14:44:12.505402: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-21 14:44:12.507784: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-21 14:44:12.511180: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-KRMLHC6): /proc/driver/nvidia/version does not exist\n",
      "2022-07-21 14:44:12.577710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset =  tf.data.Dataset.from_tensor_slices((inp, target)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'La oficina nueva es m\\xc3\\xa1s amplia.' b'Hoy Tom no quiere ver nada.'\n",
      " b'\\xc2\\xbfEs ese un r\\xc3\\xado?' b'\\xc2\\xbfEstoy detenido?'\n",
      " b'Sos un caballero.'], shape=(5,), dtype=string) \n",
      "\n",
      "tf.Tensor(\n",
      "[b'The new office is more spacious.'\n",
      " b\"Tom doesn't want to see anything today.\" b'Is this a river?'\n",
      " b'Am I under arrest?' b\"You're a gentleman.\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5], '\\n')\n",
    "    print(example_target_batch[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text)\n",
    "print(tftext.normalize_utf8(example_text, 'NFKD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters\n",
    "    text = tftext.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    # Keep space char, a to z and certain punctuations\n",
    "    # Replace with blank everything except this regex\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip extra whitespace\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp) # inp is the list of all spanish examples\n",
    "\n",
    "# FIrst 10 words from the vocab:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'que',\n",
       " 'de',\n",
       " 'el',\n",
       " 'a',\n",
       " 'no',\n",
       " 'tom',\n",
       " 'la',\n",
       " '?',\n",
       " '¿',\n",
       " 'en',\n",
       " 'es',\n",
       " 'un',\n",
       " 'se',\n",
       " 'me',\n",
       " ',',\n",
       " 'esta',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'una',\n",
       " 'mi',\n",
       " 'su',\n",
       " 'los',\n",
       " 'con',\n",
       " 'le',\n",
       " 'ella',\n",
       " 'te',\n",
       " 'para',\n",
       " 'mary',\n",
       " 'y',\n",
       " 'las',\n",
       " 'mas',\n",
       " 'tu',\n",
       " 'al',\n",
       " 'como',\n",
       " 'yo',\n",
       " 'este',\n",
       " 'estoy',\n",
       " 'muy',\n",
       " 'eso',\n",
       " 'tiene',\n",
       " 'si',\n",
       " 'del',\n",
       " 'estaba',\n",
       " 'quiero',\n",
       " 'tengo']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.get_vocabulary()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'you',\n",
       " 'tom',\n",
       " 'a',\n",
       " '?',\n",
       " 'is',\n",
       " 'he',\n",
       " 'in',\n",
       " 'of',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " ',']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")\n",
    "\n",
    "output_text_processor.adapt(target)\n",
    "output_text_processor.get_vocabulary()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'La oficina nueva es m\\xc3\\xa1s amplia.' b'Hoy Tom no quiere ver nada.'\n",
      " b'\\xc2\\xbfEs ese un r\\xc3\\xado?' b'\\xc2\\xbfEstoy detenido?'\n",
      " b'Sos un caballero.' b'Llegar\\xc3\\xa9 el s\\xc3\\xa1bado.'\n",
      " b'Todos en el pueblo lo admiran.'\n",
      " b'Mi padre siempre est\\xc3\\xa1 ocupado.'\n",
      " b'\\xc2\\xbfTom fue a la universidad?'\n",
      " b'Nos reunimos aqu\\xc3\\xad una vez por semana.' b'Ellas confiaban en ti.'\n",
      " b'No s\\xc3\\xa9 d\\xc3\\xb3nde ponerlos.'\n",
      " b'Tiene la edad suficiente como para manejar un auto.'\n",
      " b'\\xc3\\x89l derrib\\xc3\\xb3 el gran \\xc3\\xa1rbol con un hacha.'\n",
      " b'\\xc2\\xbfEst\\xc3\\xa1s en Par\\xc3\\xads?' b'Ella es culpable de robo.'\n",
      " b'Tenemos que encontrar a la persona adecuada para cada puesto.'\n",
      " b'No entiendo la estrategia de Tom.' b'Escribe con tinta.'\n",
      " b'Esta es tu habitaci\\xc3\\xb3n.'\n",
      " b'La ni\\xc3\\xb1ita tiene una mu\\xc3\\xb1eca en sus manos.'\n",
      " b'Sost\\xc3\\xa9n la puerta.' b'El le\\xc3\\xb3n es el rey de los animales.'\n",
      " b'Puedo conseguirte uno si quieres.'\n",
      " b'\\xc2\\xbfCon qui\\xc3\\xa9n vas a cenar?'\n",
      " b'Un tercio de la superficie terrestre es desierto.'\n",
      " b'Siempre desayuno frutas y avena.'\n",
      " b'Tom trat\\xc3\\xb3 de distraer a los guardias para que Mar\\xc3\\xada tuviera una oportunidad de escapar.'\n",
      " b'Es importante para los j\\xc3\\xb3venes de hoy el estudiar ingl\\xc3\\xa9s.'\n",
      " b'Dejo los libros aqu\\xc3\\xad.' b'A\\xc3\\xban podemos llegar a tiempo.'\n",
      " b'Thomas Edison invent\\xc3\\xb3 la bombilla.' b'Esto no puede ser bueno.'\n",
      " b'Tom siente un profundo cari\\xc3\\xb1o por Mary.'\n",
      " b'Lo est\\xc3\\xa1s haciendo bien.'\n",
      " b'La adicci\\xc3\\xb3n a las drogas es un c\\xc3\\xa1ncer en la sociedad moderna.'\n",
      " b'Yo lo usaba cuando estaba en China el verano pasado.'\n",
      " b'Ellos est\\xc3\\xa1n por empezar.'\n",
      " b'\\xc2\\xbfPuedes ayudarme a levantar esto?'\n",
      " b'Quiz\\xc3\\xa1 me merezco esto.' b'Hoy vuelve de S\\xc3\\xaddney.'\n",
      " b'Tom cometi\\xc3\\xb3 algunos errores en la prueba.'\n",
      " b'Por favor, d\\xc3\\xa9jame pagar.' b'Solo el tiempo lo dir\\xc3\\xa1.'\n",
      " b'Tenga usted cuidado porque se inflama muy f\\xc3\\xa1cilmente.'\n",
      " b'\\xc2\\xbfPor qu\\xc3\\xa9 quer\\xc3\\xa9s saber lo que estoy pensando?'\n",
      " b'Ese chico est\\xc3\\xa1 enamorado de su profesora.'\n",
      " b'Tom quer\\xc3\\xada ver a Mar\\xc3\\xada.'\n",
      " b'\\xc3\\x89l se distanci\\xc3\\xb3 de la pol\\xc3\\xadtica.'\n",
      " b'Tom est\\xc3\\xa1 respondiendo bien al tratamiento.'\n",
      " b'Necesito treinta minutos.' b'Los bomberos han apagado el fuego.'\n",
      " b'Es mejor no tocarlo.' b'Tom pronto alcanz\\xc3\\xb3 a Mary.'\n",
      " b'Le llevar\\xc3\\xa9 esto a Tom.'\n",
      " b'Tom pidi\\xc3\\xb3 a Mary que le ayudase a encontrar a John.'\n",
      " b'Viniste aqu\\xc3\\xad porque quer\\xc3\\xadas saber la verdad.'\n",
      " b'Sab\\xc3\\xada que Tom llegar\\xc3\\xada al final.'\n",
      " b'Estoy preocupado por la salud de mi madre.'\n",
      " b'Me han robado el pasaporte.'\n",
      " b'Si no puedo confiar en m\\xc3\\xad mismo, \\xc2\\xbfen qui\\xc3\\xa9n puedo confiar entonces?'\n",
      " b'Eso durar\\xc3\\xa1.' b'\\xc2\\xbfSabes algo acerca de esto?'\n",
      " b'Sab\\xc3\\xa9s que odio las reuniones.'], shape=(64,), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 18), dtype=int64, numpy=\n",
       "array([[   2,   11,  389,  307,   15,   35,    1,    4,    3,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,  107,   10,    9,  106,  116,   69,    4,    3,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,   13,   15,   97,   16,  383,   12,    3,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,   13,   41, 3297,   12,    3,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example_input_batch)\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "# the number of example sentences and the length of each sentence\n",
    "# extra lengths are padded with 0s\n",
    "example_tokens[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  2  11 389 307  15  35   1   4   3   0   0   0   0   0   0   0   0   0], shape=(18,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[START] la oficina nueva es mas [UNK] . [END]         '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from tokens back to IDs\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "print(example_tokens[0])\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a map and  a mask.\n",
    "# y axis represents the different strings,\n",
    "# x axis represents the word in the strings\n",
    "# the activation of a cell represents the magnitude of the token\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title(\"Token IDs\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(example_tokens!=0)\n",
    "plt.title(\"Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder/decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding1 = tf.keras.layers.Embedding(\n",
    "            self.input_vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        # The GRU layer proceses these embedding vectors as sequences\n",
    "        self.gru1 = tf.keras.layers.GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        vectors = self.embedding1(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # The GRU processes the embedding sequence\n",
    "        # Output Shape: (batch, s, enc_units)\n",
    "        # State shape: (batch, enc_units)\n",
    "        output, state = self.gru1(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape (batch): (64,)\n",
      "Input batch tokens shape (batch, s): (64, 18)\n",
      "Encoder output shape (batch, s, units): (64, 18, 1024)\n",
      "Encoder state shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = Encoder(\n",
    "    input_text_processor.vocabulary_size(),\n",
    "    embedding_dim,\n",
    "    units\n",
    ")\n",
    "\n",
    "# Get encoder output and state by passing the example through the encoder\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attention_units'))\n",
    "\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attention_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer_test = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 18])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units): (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 18)\n"
     ]
    }
   ],
   "source": [
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "context_vector, attention_weights = attention_layer_test(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0)\n",
    ")\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units): {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THe attention weights should sum to 1 for each sequence\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title(\"Attention weights\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(example_tokens!=0)\n",
    "plt.title(\"Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 2, 18])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding1 = tf.keras.layers.Embedding(\n",
    "            self.output_vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        self.gru1 = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "        self.attention1 = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        self.Wc = tf.keras.layers.Dense(\n",
    "            dec_units,\n",
    "            activation=tf.math.tanh,\n",
    "            use_bias=False\n",
    "        )\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def call(self,\n",
    "            inputs: DecoderInput,\n",
    "            state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "        if state is not None:\n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Lookup the embeddings\n",
    "        vectors = self.embedding1(inputs.new_tokens)\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "        # Process one step with the RNN\n",
    "        rnn_output, state = self.gru1(vectors, initial_state=state)\n",
    "\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Use the RNN output as the query for the attention over the encoder\n",
    "        # output\n",
    "\n",
    "        context_vector, attention_weights = self.attention1(\n",
    "            query=rnn_output,\n",
    "            value=inputs.enc_output,\n",
    "            mask=inputs.mask\n",
    "        )\n",
    "        \n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "        \n",
    "        # Join the context vector and rnn_output\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "        \n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "        logits = self.fc(attention_vector)\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    output_text_processor.vocabulary_size(),\n",
    "    embedding_dim,\n",
    "    units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder takes 4 inputs.\n",
    "\n",
    "* new_tokens - The last token generated. Initialize the decoder with the \"[START]\" token.\n",
    "* enc_output - Generated by the Encoder.\n",
    "* mask - A boolean tensor indicating where tokens != 0\n",
    "* state - The previous state output from the decoder (the internal state of the decoder's RNN). Pass None to zero-initialize it. The original paper initializes it from the encoder's final RNN state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1), dtype=int32, numpy=\n",
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
    "first_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "State shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs=DecoderInput(\n",
    "        new_tokens=first_token,\n",
    "        enc_output=example_enc_output,\n",
    "        mask=(example_tokens!=0)\n",
    "    ),\n",
    "    state=example_enc_state\n",
    ")\n",
    "\n",
    "print(f\"Logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}\")\n",
    "print(f\"State shape: (batch_size, dec_units) {dec_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['tall'],\n",
       "       ['cooks'],\n",
       "       ['buildings'],\n",
       "       ['keep'],\n",
       "       ['lawyers']], dtype='<U16')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "print(first_word.shape)\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['seemed'],\n",
       "       ['nap'],\n",
       "       ['rain'],\n",
       "       ['keys'],\n",
       "       ['match']], dtype='<U16')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "    example_enc_output,\n",
    "    mask=(example_tokens!=0)\n",
    "    ),\n",
    "    state=dec_state\n",
    ")\n",
    "sampled_token = tf.random.categorical(dec_result.logits[:,0,:], num_samples=1)\n",
    "first_word =  vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "            reduction='none'\n",
    "        )\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, ('batch', 't'))\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "        # mask off the losses on padding - meaningless\n",
    "        mask = tf.cast(y_true!=0, tf.float32)\n",
    "        shape_checker(mask, ('batch', 't'))\n",
    "        loss *= mask\n",
    "\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(\n",
    "                self,\n",
    "                embedding_dim,\n",
    "                units,\n",
    "                input_text_processor,\n",
    "                output_text_processor,\n",
    "                use_tf_function=True\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder = Encoder(\n",
    "            input_text_processor.vocabulary_size(),\n",
    "            embedding_dim,\n",
    "            units\n",
    "        )\n",
    "        decoder = Decoder(\n",
    "            output_text_processor.vocabulary_size(),\n",
    "            embedding_dim,\n",
    "            units\n",
    "        )\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        self.shape_checker = ShapeChecker()\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "    \n",
    "    def _preprocess(self, input_text, target_text):\n",
    "        self.shape_checker(input_text, ('batch',))\n",
    "        self.shape_checker(target_text, ('batch', ))\n",
    "\n",
    "        # Convert the text to token IDs\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        target_tokens = self.output_text_processor(target_text)\n",
    "        self.shape_checker(input_tokens, ('batch', 's'))\n",
    "        self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "        input_mask = input_tokens!=0\n",
    "        self.shape_checker(input_mask, ('batch', 's'))\n",
    "        target_mask = target_tokens!=0\n",
    "        self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "        \n",
    "    def _train_step(self, inputs):\n",
    "        input_text, target_text = inputs\n",
    "\n",
    "        (input_tokens, input_mask,\n",
    "        target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "        max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode the input\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "            self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "            self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "            # init dec state to the encoder's final state\n",
    "            # this only works if enc and dec have the same number of units\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            for t in tf.range(max_target_length-1):\n",
    "                # Pass the current input to the decoder and target for\n",
    "                # the next prediction. These will be two tokens from the\n",
    "                # target sequence\n",
    "                new_tokens = target_tokens[:,t:t+2]\n",
    "                step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                        enc_output, dec_state)\n",
    "                \n",
    "                loss = loss + step_loss\n",
    "\n",
    "            avg_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "        # Optimization step\n",
    "        gradients = tape.gradient(avg_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {'batch_loss': avg_loss}\n",
    "\n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "        decoder_input = DecoderInput(\n",
    "            new_tokens=input_token,\n",
    "            enc_output=enc_output,\n",
    "            mask=input_mask\n",
    "        )\n",
    "\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "        self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "        self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "        self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "        y = target_token\n",
    "        y_pred = dec_result.logits\n",
    "        step_loss = self.loss(y, y_pred)\n",
    "\n",
    "        return step_loss, dec_state\n",
    "    \n",
    "    @tf.function\n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim,\n",
    "    units,\n",
    "    input_text_processor,\n",
    "    output_text_processor,\n",
    "    use_tf_function=False\n",
    ")\n",
    "\n",
    "translator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=MaskedLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.517193191416238"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(output_text_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.590484>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5590606>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5014906>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.3413143>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.791249>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.049718>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.767887>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.253486>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.088516>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9560626>}\n",
      "\n",
      "\n",
      "CPU times: user 4min 36s, sys: 22.7 s, total: 4min 59s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.965217>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.train_step([example_input_batch, example_target_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9463172>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.8868127>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.8438866>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.7469192>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.6618872>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.6316411>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.5906389>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.573899>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.56108>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.5168557>}\n",
      "\n",
      "\n",
      "CPU times: user 2min 56s, sys: 5 s, total: 3min 1s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "for n in range(100):\n",
    "    print(n+1, end='')\n",
    "    logs = translator.train_step([example_input_batch, example_target_batch])\n",
    "    losses.append(logs['batch_loss'].numpy())\n",
    "    print(\"\\n\")\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0613189"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim,\n",
    "    units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor\n",
    ")\n",
    "\n",
    "train_translator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=MaskedLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_logs_callback = BatchLogsCallback('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   9/1859 [..............................] - ETA: 1:25:25 - batch_loss: 6.4050"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb Cell 60\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb#ch0000059vscode-remote?line=0'>1</a>\u001b[0m train_translator\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb#ch0000059vscode-remote?line=1'>2</a>\u001b[0m     dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb#ch0000059vscode-remote?line=2'>3</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb#ch0000059vscode-remote?line=3'>4</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[batch_logs_callback],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb#ch0000059vscode-remote?line=4'>5</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/TestABC/KosmischeEinsamkeit/Development/AI_ML/seq2seq-Translator/translator.ipynb#ch0000059vscode-remote?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_translator.fit(\n",
    "    dataset,\n",
    "    epochs=3,\n",
    "    callbacks=[batch_logs_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch_logs_callback.logs)\n",
    "plt.ylim([0,3])\n",
    "plt.xlabel('Batch no.')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            input_text_processor,\n",
    "            output_text_processor\n",
    "            ):\n",
    "        self.encoder - encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        \n",
    "        self.output_token_string_from_indices = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        indices_from_string = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.output_text_processor.get_vocabulary(),\n",
    "                mask_token=''\n",
    "            )\n",
    "        )\n",
    "\n",
    "        token_mask_ids = indices_from_string(['', '[UNK]', '[START]', '[END]']).numpy()\n",
    "        token_mask = np.zeros([indices_from_string.vocabulary_size()], dtype=bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask- token_mask\n",
    "\n",
    "        self.start_token_index = indices_from_string(tf.constant('[START]'))\n",
    "        self.end_token_index = indices_from_string(tf.constant('[END]'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=train_translator.input_text_processor,\n",
    "    output_text_processor=train_translator.output_text_processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(result_tokens, ('batch', 't'))\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "    shape_checker(result_text_tokens, ('batch', 't'))\n",
    "\n",
    "    result_text = tf.strings.reduce_join(\n",
    "        result_text_tokens,\n",
    "        axis=1,\n",
    "        seperator=' '\n",
    "    )\n",
    "\n",
    "    shape_checker(result_text, ('batch'))\n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    shape_checker(result_text, ('batch',))\n",
    "\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_tokens = tf.random.uniform(\n",
    "    shape=[5, 2],\n",
    "    minval=0,\n",
    "    dtype=tf.int64,\n",
    "    maxval=output_text_processor.vocabulary_size()\n",
    ")\n",
    "\n",
    "translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "    shape_checker = ShapeChecker()\n",
    "    # t is usually 1 here\n",
    "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
    "    shape_checker(self.token_mask, ('vocab',))\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
    "\n",
    "    # Set the logits for all masked tokens to -infinity\n",
    "    # so they are never chosen\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = tf.squeeze(logits, axis=-1)\n",
    "        new_tokens = tf.random.categorical(logits / temperature, num_samples=1)\n",
    "\n",
    "    shape_checker(new_tokens, ('batch', 't'))\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.sample = sample\n",
    "\n",
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
    "example_output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete text-to-text translation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_unrolled(\n",
    "    self,\n",
    "    input_text,\n",
    "    *,\n",
    "    max_length=50,\n",
    "    return_attention=True,\n",
    "    temperature=1.0\n",
    "    ):\n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_tokens = []\n",
    "    attention = []\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        dec_input = DecoderInput(\n",
    "            new_tokens=new_tokens,\n",
    "            enc_output=enc_output,\n",
    "            mask=(input_tokens!=0)\n",
    "        )\n",
    "\n",
    "        dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "        attention.append(dec_result.attention_weights)\n",
    "\n",
    "        new_tokens = self.sample(dec_result.attention_weights)\n",
    "\n",
    "        # If a sequence produces an 'END' token set it 'done'\n",
    "        done = done | (new_tokens == self.end_token)0\n",
    "        # Once a sequence is done it only produces 0-padding\n",
    "        new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "        # Collect the generated tokens\n",
    "        result_tokens.append(new_tokens)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "        # Convert the list of generated token IDs to a list of strings\n",
    "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "        result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "        if return_attention:\n",
    "            attention_stack = tf.concat(attention, axis=1)\n",
    "            return {'text': result_text, 'attention': attention_stack}\n",
    "        else:\n",
    "            return {'text': result_text}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('seq2seq-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b1576f7c6a75cfe22f197e87d4cc49d76ff0c6690cc3efd57128c1cf81b92ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
