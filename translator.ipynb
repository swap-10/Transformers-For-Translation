{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 12:36:41.925310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-05 12:36:41.925426: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tftext\n",
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Chekcer\n",
    "\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        self.shapes = dict()\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "        \n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "        \n",
    "        if rank != len(names):\n",
    "            raise ValueError(f\"Rank mismatch:\\n\"\n",
    "                             f\"     found {rank}: {shape.numpy()}\\n\"\n",
    "                             f\"     expected {len(names)}: {names}\"\n",
    "                             )\n",
    "        \n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "            \n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"     found: {new_dim}\\n\"\n",
    "                                 f\"     expected: {old_dim}\\n\"\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    cache_subdir=pathlib.Path.cwd(),\n",
    "    cache_dir=pathlib.Path.cwd(),\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for target, inp in pairs]\n",
    "    target = [target for target, input in pairs]\n",
    "\n",
    "    return target, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "target, inp = load_data(path_to_file)\n",
    "print(inp[-1], '\\n', target[-1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 12:37:21.685368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-05 12:37:21.685470: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-05 12:37:21.685978: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-KRMLHC6): /proc/driver/nvidia/version does not exist\n",
      "2022-07-05 12:37:21.689385: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset =  tf.data.Dataset.from_tensor_slices((inp, target)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Dicen que Venecia es una hermosa ciudad.'\n",
      " b'\\xc3\\x89l no est\\xc3\\xa1 seguro de querer hacerlo.'\n",
      " b'Todav\\xc3\\xada no se ha le\\xc3\\xaddo el libro.'\n",
      " b'\\xc2\\xbfAlguien est\\xc3\\xa1 herido?'\n",
      " b'Aprender una lengua extranjera requiere mucho tiempo.'], shape=(5,), dtype=string) \n",
      "\n",
      "tf.Tensor(\n",
      "[b'They say that Venice is a beautiful city.'\n",
      " b\"He's not sure he wants to do this.\" b\"He hasn't read the book yet.\"\n",
      " b'Anyone hurt?' b'To learn a foreign language requires a lot of time.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5], '\\n')\n",
    "    print(example_target_batch[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text)\n",
    "print(tftext.normalize_utf8(example_text, 'NFKD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters\n",
    "    text = tftext.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    # Keep space char, a to z and certain punctuations\n",
    "    # Replace with blank everything except this regex\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip extra whitespace\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp) # inp is the list of all spanish examples\n",
    "\n",
    "# FIrst 10 words from the vocab:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'que',\n",
       " 'de',\n",
       " 'el',\n",
       " 'a',\n",
       " 'no',\n",
       " 'tom',\n",
       " 'la',\n",
       " '?',\n",
       " '¿',\n",
       " 'en',\n",
       " 'es',\n",
       " 'un',\n",
       " 'se',\n",
       " 'me',\n",
       " ',',\n",
       " 'esta',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'una',\n",
       " 'mi',\n",
       " 'su',\n",
       " 'los',\n",
       " 'con',\n",
       " 'le',\n",
       " 'ella',\n",
       " 'te',\n",
       " 'para',\n",
       " 'mary',\n",
       " 'y',\n",
       " 'las',\n",
       " 'mas',\n",
       " 'tu',\n",
       " 'al',\n",
       " 'como',\n",
       " 'yo',\n",
       " 'este',\n",
       " 'estoy',\n",
       " 'muy',\n",
       " 'eso',\n",
       " 'tiene',\n",
       " 'si',\n",
       " 'del',\n",
       " 'estaba',\n",
       " 'quiero',\n",
       " 'tengo']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.get_vocabulary()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'you',\n",
       " 'tom',\n",
       " 'a',\n",
       " '?',\n",
       " 'is',\n",
       " 'he',\n",
       " 'in',\n",
       " 'of',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " ',']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")\n",
    "\n",
    "output_text_processor.adapt(target)\n",
    "output_text_processor.get_vocabulary()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Dicen que Venecia es una hermosa ciudad.'\n",
      " b'\\xc3\\x89l no est\\xc3\\xa1 seguro de querer hacerlo.'\n",
      " b'Todav\\xc3\\xada no se ha le\\xc3\\xaddo el libro.'\n",
      " b'\\xc2\\xbfAlguien est\\xc3\\xa1 herido?'\n",
      " b'Aprender una lengua extranjera requiere mucho tiempo.'\n",
      " b'Tom ech\\xc3\\xb3 a Mar\\xc3\\xada del bar.'\n",
      " b'El ni\\xc3\\xb1o corri\\xc3\\xb3 hacia su madre.'\n",
      " b'El a\\xc3\\xb1o pasado ocurrieron muchos accidentes automovil\\xc3\\xadsticos.'\n",
      " b'El problema es que \\xc3\\xa9l no tiene dinero.' b'Tom gan\\xc3\\xb3.'\n",
      " b'Tom\\xc3\\xa1s hoy se ve much\\xc3\\xadsimo mejor.'\n",
      " b'Compr\\xc3\\xa9 la misma c\\xc3\\xa1mara que tienes t\\xc3\\xba.'\n",
      " b'No s\\xc3\\xa9 cu\\xc3\\xa1ndo volvi\\xc3\\xb3 de Francia.'\n",
      " b'Uno de nosotros deber\\xc3\\xada hablar con Tom.'\n",
      " b'Tom escuch\\xc3\\xb3 un helic\\xc3\\xb3ptero por encima de su cabeza.'\n",
      " b'Es dif\\xc3\\xadcil para un anciano cambiar su manera de pensar.'\n",
      " b'Le haremos hablar, cueste lo que cueste.'\n",
      " b'\\xc2\\xbfQuer\\xc3\\xa9is sentaros?' b'El mundo es un pa\\xc3\\xb1uelo.'\n",
      " b'Tom nunca hizo eso aqu\\xc3\\xad.' b'Apenas pod\\xc3\\xada verlo.'\n",
      " b'Tom escap\\xc3\\xb3 del golpe por los pelos.' b'Me acabo de casar.'\n",
      " b'No quiero causar m\\xc3\\xa1s problemas.' b'Ellos dos estaban desnudos.'\n",
      " b'Mi padre es profesor de ingl\\xc3\\xa9s.'\n",
      " b'\\xc3\\x89l le teme a la muerte.'\n",
      " b'Yo conoc\\xc3\\xad a un joven cuyo nombre era Tom.'\n",
      " b'Trae un balde de manzanas.'\n",
      " b'El f\\xc3\\xbatbol es m\\xc3\\xa1s popular que el tenis.'\n",
      " b'No puedo hacerlo sin un poco de ayuda.'\n",
      " b'\\xc3\\x89l admir\\xc3\\xb3 mi auto nuevo.' b'Eso huele bien.'\n",
      " b'La culpa es m\\xc3\\xada.'\n",
      " b'Los vaqueros tardan una eternidad en secarse.' b'La cogieron.'\n",
      " b'No tenemos m\\xc3\\xa1s opci\\xc3\\xb3n que proseguir.' b'Coge una.'\n",
      " b'\\xc2\\xbfVes gente en el parque?' b'Quiero ser ingeniero.'\n",
      " b'Corrimos alrededor del parque.'\n",
      " b'Hoy en d\\xc3\\xada mucha gente viaja en coche.'\n",
      " b'Estoy feliz de verte otra vez.'\n",
      " b'T\\xc3\\xba no te levantas tan pronto como tu hermana.'\n",
      " b'Deciles la verdad.' b'\\xc2\\xbfEs eso otra broma?'\n",
      " b'Hay una agradable brisa aqu\\xc3\\xad.'\n",
      " b'\\xc2\\xbfQu\\xc3\\xa9 desayunaste?' b'\\xc2\\xbfC\\xc3\\xb3mo decidisteis?'\n",
      " b'Hay que cocer las casta\\xc3\\xb1as por al menos quince minutos.'\n",
      " b'Los ni\\xc3\\xb1os estaban en fila.' b'Esperaba sorprender a Tom.'\n",
      " b'No sab\\xc3\\xada que ten\\xc3\\xadas compa\\xc3\\xb1\\xc3\\xada.'\n",
      " b'Se lo expliqu\\xc3\\xa9.' b'Hoy estoy ocupado.'\n",
      " b'Ayer, cuando Tom estaba dando marcha atr\\xc3\\xa1s con su coche, atropell\\xc3\\xb3 la bicicleta de Mary.'\n",
      " b'No dejes tus pertenencias desatendidas en el playa.'\n",
      " b'\\xc2\\xbfD\\xc3\\xb3nde los pusiste?' b'Eso es un mito.'\n",
      " b'Nosotros sentimos que se sacud\\xc3\\xada el suelo.'\n",
      " b'No puedo adivinar qu\\xc3\\xa9 quiere en realidad.'\n",
      " b'Esper\\xc3\\xa9 en vano toda la tarde.' b'No tengo qu\\xc3\\xa9 ponerme.'\n",
      " b'Helen Keller era ciega, sorda y muda.'], shape=(64,), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 20), dtype=int64, numpy=\n",
       "array([[   2,  869,    5,    1,   15,   23,  779,  222,    4,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,    7,    9,   20,  206,    6, 1982,  198,    4,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,  147,    9,   17,   61,  927,    7,  105,    4,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,   13,  155,   20, 1093,   12,    3,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example_input_batch)\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "# the number of example sentences and the length of each sentence\n",
    "# extra lengths are padded with 0s\n",
    "example_tokens[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  2 869   5   1  15  23 779 222   4   3   0   0   0   0   0   0   0   0\n",
      "   0   0], shape=(20,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[START] dicen que [UNK] es una hermosa ciudad . [END]          '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from tokens back to IDs\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "print(example_tokens[0])\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a map and  a mask.\n",
    "# y axis represents the different strings,\n",
    "# x axis represents the word in the strings\n",
    "# the activation of a cell represents the magnitude of the token\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title(\"Token IDs\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(example_tokens!=0)\n",
    "plt.title(\"Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder/decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding1 = tf.keras.layers.Embedding(\n",
    "            self.input_vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        # The GRU layer proceses these embedding vectors as sequences\n",
    "        self.gru1 = tf.keras.layers.GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        vectors = self.embedding1(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # The GRU processes the embedding sequence\n",
    "        # Output Shape: (batch, s, enc_units)\n",
    "        # State shape: (batch, enc_units)\n",
    "        output, state = self.gru1(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape (batch): (64,)\n",
      "Input batch tokens shape (batch, s): (64, 20)\n",
      "Encoder output shape (batch, s, units): (64, 20, 1024)\n",
      "Encoder state shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = Encoder(\n",
    "    input_text_processor.vocabulary_size(),\n",
    "    embedding_dim,\n",
    "    units\n",
    ")\n",
    "\n",
    "# Get encoder output and state by passing the example through the encoder\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attention_units'))\n",
    "\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attention_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer_test = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units): (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 20)\n"
     ]
    }
   ],
   "source": [
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "context_vector, attention_weights = attention_layer_test(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0)\n",
    ")\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units): {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THe attention weights should sum to 1 for each sequence\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title(\"Attention weights\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(example_tokens!=0)\n",
    "plt.title(\"Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 2, 20])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding1 = tf.keras.layers.Embedding(\n",
    "            self.output_vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        self.gru1 = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "        self.attention1 = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        self.Wc = tf.keras.layers.Dense(\n",
    "            dec_units,\n",
    "            activation=tf.math.tanh,\n",
    "            use_bias=False\n",
    "        )\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def call(self,\n",
    "            inputs: DecoderInput,\n",
    "            state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "        if state is not None:\n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Lookup the embeddings\n",
    "        vectors = self.embedding1(inputs.new_tokens)\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "        # Process one step with the RNN\n",
    "        rnn_output, state = self.gru1(vectors, initial_state=state)\n",
    "\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Use the RNN output as the query for the attention over the encoder\n",
    "        # output\n",
    "\n",
    "        context_vector, attention_weights = self.attention1(\n",
    "            query=rnn_output,\n",
    "            value=inputs.enc_output,\n",
    "            mask=inputs.mask\n",
    "        )\n",
    "        \n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "        \n",
    "        # Join the context vector and rnn_output\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "        \n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "        logits = self.fc(attention_vector)\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    output_text_processor.vocabulary_size(),\n",
    "    embedding_dim,\n",
    "    units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder takes 4 inputs.\n",
    "\n",
    "* new_tokens - The last token generated. Initialize the decoder with the \"[START]\" token.\n",
    "* enc_output - Generated by the Encoder.\n",
    "* mask - A boolean tensor indicating where tokens != 0\n",
    "* state - The previous state output from the decoder (the internal state of the decoder's RNN). Pass None to zero-initialize it. The original paper initializes it from the encoder's final RNN state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1), dtype=int32, numpy=\n",
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
    "first_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "State shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs=DecoderInput(\n",
    "        new_tokens=first_token,\n",
    "        enc_output=example_enc_output,\n",
    "        mask=(example_tokens!=0)\n",
    "    ),\n",
    "    state=example_enc_state\n",
    ")\n",
    "\n",
    "print(f\"Logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}\")\n",
    "print(f\"State shape: (batch_size, dec_units) {dec_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['only'],\n",
       "       ['glanced'],\n",
       "       ['production'],\n",
       "       ['national'],\n",
       "       ['bar']], dtype='<U16')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "print(first_word.shape)\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['bottles'],\n",
       "       ['facts'],\n",
       "       ['crash'],\n",
       "       ['impact'],\n",
       "       ['operation']], dtype='<U16')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "    example_enc_output,\n",
    "    mask=(example_tokens!=0)\n",
    "    ),\n",
    "    state=dec_state\n",
    ")\n",
    "sampled_token = tf.random.categorical(dec_result.logits[:,0,:], num_samples=1)\n",
    "first_word =  vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "            reduction='none'\n",
    "        )\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, ('batch', 't'))\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "        # mask off the losses on padding - meaningless\n",
    "        mask = tf.cast(y_true!=0, tf.float32)\n",
    "        shape_checker(mask, ('batch', 't'))\n",
    "        loss *= mask\n",
    "\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(\n",
    "                self,\n",
    "                embedding_dim,\n",
    "                units,\n",
    "                input_text_processor,\n",
    "                output_text_processor,\n",
    "                use_tf_function=True\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder = Encoder(\n",
    "            input_text_processor.vocabulary_size(),\n",
    "            embedding_dim,\n",
    "            units\n",
    "        )\n",
    "        decoder = Decoder(\n",
    "            output_text_processor.vocabulary_size(),\n",
    "            embedding_dim,\n",
    "            units\n",
    "        )\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        self.shape_checker = ShapeChecker()\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "    \n",
    "    def _preprocess(self, input_text, target_text):\n",
    "        self.shape_checker(input_text, ('batch',))\n",
    "        self.shape_checker(target_text, ('batch', ))\n",
    "\n",
    "        # Convert the text to token IDs\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        target_tokens = self.output_text_processor(target_text)\n",
    "        self.shape_checker(input_tokens, ('batch', 's'))\n",
    "        self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "        input_mask = input_tokens!=0\n",
    "        self.shape_checker(input_mask, ('batch', 's'))\n",
    "        target_mask = target_tokens!=0\n",
    "        self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "        \n",
    "    def _train_step(self, inputs):\n",
    "        input_text, target_text = inputs\n",
    "\n",
    "        (input_tokens, input_mask,\n",
    "        target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "        max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode the input\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "            self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "            self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "            # init dec state to the encoder's final state\n",
    "            # this only works if enc and dec have the same number of units\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            for t in tf.range(max_target_length-1):\n",
    "                # Pass the current input to the decoder and target for\n",
    "                # the next prediction. These will be two tokens from the\n",
    "                # target sequence\n",
    "                new_tokens = target_tokens[:,t:t+2]\n",
    "                step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                        enc_output, dec_state)\n",
    "                \n",
    "                loss = loss + step_loss\n",
    "\n",
    "            avg_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "        # Optimization step\n",
    "        gradients = tape.gradient(avg_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {'batch_loss': avg_loss}\n",
    "\n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "        decoder_input = DecoderInput(\n",
    "            new_tokens=input_token,\n",
    "            enc_output=enc_output,\n",
    "            mask=input_mask\n",
    "        )\n",
    "\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "        self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "        self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "        self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "        y = target_token\n",
    "        y_pred = dec_result.logits\n",
    "        step_loss = self.loss(y, y_pred)\n",
    "\n",
    "        return step_loss, dec_state\n",
    "    \n",
    "    @tf.function\n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim,\n",
    "    units,\n",
    "    input_text_processor,\n",
    "    output_text_processor,\n",
    "    use_tf_function=False\n",
    ")\n",
    "\n",
    "translator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=MaskedLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.517193191416238"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(output_text_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.593755>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.562275>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5036016>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.329206>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.632119>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.0783997>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.562949>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.408111>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.187345>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.014921>}\n",
      "\n",
      "\n",
      "CPU times: user 5min 12s, sys: 31.8 s, total: 5min 44s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5937376>}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.train_step([example_input_batch, example_target_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5622196>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.502313>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.321965>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.589846>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.836919>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.4230723>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.235688>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.053107>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9655254>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9608147>}\n",
      "\n",
      "\n",
      "CPU times: user 3min 35s, sys: 12.8 s, total: 3min 48s\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "for n in range(100):\n",
    "    print(n+1, end='')\n",
    "    logs = translator.train_step([example_input_batch, example_target_batch])\n",
    "    losses.append(logs['batch_loss'].numpy())\n",
    "print(\"\\n\")\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4094921"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('seq2seq-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b1576f7c6a75cfe22f197e87d4cc49d76ff0c6690cc3efd57128c1cf81b92ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
