{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tftext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Chekcer\n",
    "\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        self.shapes = dict()\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "        \n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "        \n",
    "        if rank != len(names):\n",
    "            raise ValueError(f\"Rank mismatch:\\n\"\n",
    "                             f\"     found {rank}: {shape.numpy()}\\n\"\n",
    "                             f\"     expected {len(names)}: {names}\"\n",
    "                             )\n",
    "        \n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "            \n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"     found: {new_dim}\\n\"\n",
    "                                 f\"     expected: {old_dim}\\n\"\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    cache_subdir=pathlib.Path.cwd(),\n",
    "    cache_dir=pathlib.Path.cwd(),\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for target, inp in pairs]\n",
    "    target = [target for target, input in pairs]\n",
    "\n",
    "    return target, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "target, inp = load_data(path_to_file)\n",
    "print(inp[-1], '\\n', target[-1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 08:28:56.725869: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-03 08:28:56.726086: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-03 08:28:56.726168: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-KRMLHC6): /proc/driver/nvidia/version does not exist\n",
      "2022-07-03 08:28:56.730014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset =  tf.data.Dataset.from_tensor_slices((inp, target)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Hay botellas vac\\xc3\\xadas en la caja.'\n",
      " b'Siempre deber\\xc3\\xadas golpear la puerta antes de entrar a la habitaci\\xc3\\xb3n de Tom.'\n",
      " b'El perro est\\xc3\\xa1 jadeando.' b'He comprado una taza de caf\\xc3\\xa9.'\n",
      " b'Felicitaciones por su victoria en el torneo.'], shape=(5,), dtype=string) \n",
      "\n",
      "tf.Tensor(\n",
      "[b'There are some empty bottles in the box.'\n",
      " b\"You should always knock before entering Tom's room.\"\n",
      " b'The dog is panting.' b\"I've bought a cup of coffee.\"\n",
      " b'Let me congratulate you on your victory in the tournament.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5], '\\n')\n",
    "    print(example_target_batch[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text)\n",
    "print(tftext.normalize_utf8(example_text, 'NFKD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters\n",
    "    text = tftext.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    # Keep space char, a to z and certain punctuations\n",
    "    # Replace with blank everything except this regex\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip extra whitespace\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp) # inp is the list of all spanish examples\n",
    "\n",
    "# FIrst 10 words from the vocab:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'que',\n",
       " 'de',\n",
       " 'el',\n",
       " 'a',\n",
       " 'no',\n",
       " 'tom',\n",
       " 'la',\n",
       " '?',\n",
       " '¿',\n",
       " 'en',\n",
       " 'es',\n",
       " 'un',\n",
       " 'se',\n",
       " 'me',\n",
       " ',',\n",
       " 'esta',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'una',\n",
       " 'mi',\n",
       " 'su',\n",
       " 'los',\n",
       " 'con',\n",
       " 'le',\n",
       " 'ella',\n",
       " 'te',\n",
       " 'para',\n",
       " 'mary',\n",
       " 'y',\n",
       " 'las',\n",
       " 'mas',\n",
       " 'tu',\n",
       " 'al',\n",
       " 'como',\n",
       " 'yo',\n",
       " 'este',\n",
       " 'estoy',\n",
       " 'muy',\n",
       " 'eso',\n",
       " 'tiene',\n",
       " 'si',\n",
       " 'del',\n",
       " 'estaba',\n",
       " 'quiero',\n",
       " 'tengo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.get_vocabulary()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'you',\n",
       " 'tom',\n",
       " 'a',\n",
       " '?',\n",
       " 'is',\n",
       " 'he',\n",
       " 'in',\n",
       " 'of',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " ',']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")\n",
    "\n",
    "output_text_processor.adapt(target)\n",
    "output_text_processor.get_vocabulary()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Hay botellas vac\\xc3\\xadas en la caja.'\n",
      " b'Siempre deber\\xc3\\xadas golpear la puerta antes de entrar a la habitaci\\xc3\\xb3n de Tom.'\n",
      " b'El perro est\\xc3\\xa1 jadeando.' b'He comprado una taza de caf\\xc3\\xa9.'\n",
      " b'Felicitaciones por su victoria en el torneo.'\n",
      " b'No me gusta salir cuando est\\xc3\\xa1 oscuro.'\n",
      " b'Se mont\\xc3\\xb3 en su coche.'\n",
      " b'O sea, que nuestros esfuerzos fueron para nada.'\n",
      " b'Son demasiado peligrosos.' b'Hagamos el trabajo.'\n",
      " b'Cree el ladr\\xc3\\xb3n que todos son de su condici\\xc3\\xb3n.'\n",
      " b'Partir una tarta en partes iguales es bastante complicado.'\n",
      " b'Conseguir\\xc3\\xa9 la direcci\\xc3\\xb3n de Tom.'\n",
      " b'Esta m\\xc3\\xbasica es de los cuarentas.'\n",
      " b'Los aspectos t\\xc3\\xadpicos de una cultura ajena cambian por los aspectos t\\xc3\\xadpicos de la cultura local de los traductores.'\n",
      " b'Tom sabe demasiado.'\n",
      " b'Un per\\xc3\\xadodo prolongado de tiempo lluvioso es perjudicial para las plantas.'\n",
      " b'Son muy valientes.'\n",
      " b'Le escribo para expresarle mi insatisfacci\\xc3\\xb3n.'\n",
      " b'El aburrimiento era lo que Aldous Huxley consideraba como una de las condiciones humanas m\\xc3\\xa1s peligrosas.'\n",
      " b'Preguntar\\xc3\\xa9 m\\xc3\\xa1s tarde.'\n",
      " b'No deber\\xc3\\xadas hablar tan mal de \\xc3\\xa9l.'\n",
      " b'Est\\xc3\\xa1n pagando el alquiler.'\n",
      " b'La levadura hace que la cerveza fermente.'\n",
      " b'\\xc2\\xbfA qu\\xc3\\xa9 hora deber\\xc3\\xada ir al aeropuerto?'\n",
      " b'Te dir\\xc3\\xa9 en cuanto lleguemos ah\\xc3\\xad.'\n",
      " b'Obviamente, hay un problema.'\n",
      " b'Este libro es m\\xc3\\xa1s viejo que aquel.'\n",
      " b'No s\\xc3\\xa9 cocinar tan bien como mi madre.' b'Tom no es religioso.'\n",
      " b'Traje vino.'\n",
      " b'De acuerdo a mis c\\xc3\\xa1lculos, ella deber\\xc3\\xada estar en India por ahora.'\n",
      " b'No deber\\xc3\\xadas juzgar a una persona s\\xc3\\xb3lo por su apariencia.'\n",
      " b'\\xc2\\xbfQu\\xc3\\xa9 bebe usted por lo general, vino o cerveza?'\n",
      " b'\\xc2\\xbfD\\xc3\\xb3nde est\\xc3\\xa1 todo el mundo?'\n",
      " b'El \\xc3\\xbanico superviviente del accidente fue un beb\\xc3\\xa9.'\n",
      " b'Anoche hab\\xc3\\xada luna llena.' b'El cachorro quiere dormir.'\n",
      " b'Casi todas las personas en este pa\\xc3\\xads son biling\\xc3\\xbces.'\n",
      " b'Los b\\xc3\\xbahos son monos.'\n",
      " b'Comprendo que es dif\\xc3\\xadcil de creer.'\n",
      " b'No quiero agobiarlo con mis problemas.'\n",
      " b'No conf\\xc3\\xades en \\xc3\\xa9l.'\n",
      " b'No puedo soportar m\\xc3\\xa1s el dolor.' b'Ellos nos hicieron pagar.'\n",
      " b'\\xc2\\xbfEse es un bus o un auto?'\n",
      " b'\\xc3\\x89l dise\\xc3\\xb1\\xc3\\xb3 el coche.'\n",
      " b'Tom no parece estar dispuesto a abordar el problema.'\n",
      " b'Esto es lo que tienen que hacer.' b'\\xc3\\x89l se considera suertudo.'\n",
      " b'Queremos que cantes una canci\\xc3\\xb3n.' b'No parece que haya prisa.'\n",
      " b'Nos ense\\xc3\\xb1a franc\\xc3\\xa9s.'\n",
      " b'Compr\\xc3\\xa9 esto cuando estaba en Estados Unidos.'\n",
      " b'Tengo una pecera.' b'Las ratas no le gustan a nadie.'\n",
      " b'La hierba al otro lado de la colina est\\xc3\\xa1 siempre m\\xc3\\xa1s verde.'\n",
      " b'Me acuerdo bien de Tom.'\n",
      " b'Tom dej\\xc3\\xb3 sola a Mar\\xc3\\xada en la habitaci\\xc3\\xb3n.'\n",
      " b'Tienes que cuidar de tu perro.'\n",
      " b'Nuestra oficina se ubica en el centro de la cuidad.'\n",
      " b'\\xc2\\xbfEso fue un si?' b'Es de vital importancia.'\n",
      " b'\\xc3\\x89l es bueno tomando fotos.'], shape=(64,), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 15), dtype=int64, numpy=\n",
       "array([[   2,   59, 2911, 4316,   14,   11,  443,    4,    3,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [   2,  100,  175, 4457,   11,  179,  130,    6,  585,    8,   11,\n",
       "         173,    6,   10,    4],\n",
       "       [   2,    7,  144,   20,    1,    4,    3,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [   2,   74,  910,   23,  736,    6,  240,    4,    3,    0,    0,\n",
       "           0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example_input_batch)\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "# the number of example sentences and the length of each sentence\n",
    "# extra lengths are padded with 0s\n",
    "example_tokens[:4, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   2   59 2911 4316   14   11  443    4    3    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(22,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[START] hay botellas vacias en la caja . [END]             '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from tokens back to IDs\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "print(example_tokens[0])\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcYElEQVR4nO3dfZRdVXnH8e8zdyaZJBMSMoSQTBLCSwQjLF5MA75UEOob0hK7LNVSm9Z0pa22ta1aaWVRX9ouXFqxXVprFGtUBBFwQW0rQgpSl7wGMIGkEogE8joQEpJAILkzT/+4Z1yXcPeZO2fuOffsmd9nrVl37nnmnLNz88yefZ+7z9nm7oiISHw62t0AERHJRh24iEik1IGLiERKHbiISKTUgYuIREoduIhIpNSB58jMzjWzLe1uh0hszOwOM/vDdrej7NSBN8nM9td9DZrZgbrnl7S5bb9M9uSPxmBd27aY2XVm9ivtbKOMPWb2hJkdNLOjDtv+oJm5mS1oU9PGDXXgTXL3nqEv4Eng1+u2Xd3u9h1mW9LOqcDZwP8B/2tm57e3WTIG/QJ479ATMzsVmNy+5owv6sBHycwmmtkXzGxb8vUFM5sY+Nk/N7P1ZjY32e9zZvakme00s38zs0nJz52bjJw/bGb9ZrbdzP5gpG3zmi3ufjnwNeAzyfHNzK5Mjr3XzNaZ2SmjeR1k3PoW8Ht1z5cB3xx6YmbvTEbke83sKTP7RF2s28y+bWa7zGyPmd1nZrMOP4GZzTaztWb20Tz/ITFSBz56H6c2yj0dOA1YAlx2+A+Z2eXA7wPnuPsW4ArgVcl+JwJ9wOV1uxwDTEu2Lwe+ZGZHjqKdNwJnmtkU4K3Am5LzTwMuBnaN4tgyft0NHGFmrzazCvAe4Nt18eepdfDTgXcCf2JmS5PYMmr5Nw/oBf4YOFB/cDM7Dvgx8EV3/2x+/4w4qQMfvUuAT7l7v7s/DXwSeF9d3Mzs89Q6zTe7+9NmZsAK4C/d/Vl33wf8I7XkH3IoOe4hd/8vYD9w0ijauQ0war9Ih6iVV04GzN03uPv2URxbxrehUfhbgA3A1qGAu9/h7uvcfdDd1wLXAOck4UPUOu4T3X3A3de4+9664y4Cbgf+zt1XFvEPiU1nuxswBswBNtc935xsGzKdWmf92+7+XLJtJrU64ZpaXw7UOtdK3X673L1a9/wFoGcU7ewDHNjj7v9jZl8EvgQca2Y3Ah857JdHpFnfAu4EjqOufAJgZmdRe7d5CjABmAh8r26/ecC1Zjad2sj94+5+KIlfAjwGXJ9z+6OlEfjobQOOrXs+P9k2ZDdwIfDvZvaGZNsz1N4qvsbdpydf05IPHvPyLuABd38ewN3/xd1fS22U8ypA9UXJxN03U/sw8wJqpbp63wFuBua5+zTg36gNVkjeXX7S3RcBr6f2e1JfT/8Etd+V7yTlGTmMOvDRuwa4zMxmJtOpLuflNUDc/Q5qo4kbzWyJuw8CXwWuNLOjAcysz8ze1sqGJR9W9pnZ3wF/CPxtsv1XzOwsM+uiVqN8ERhs5bll3FkOnDc0QKgzFXjW3V80syXA7wwFzOzNZnZq0jnvpVZSqc/DQ8BvAVOAb5qZ+qvD6AUZvb8H7gfWAuuAB5JtL+PutwLvB/7DzM4EPkbt7eHdZrYXuI3R1bjrzTGz/dTq5vcBpwLnuvuPkvgR1P6A7KZW8tkF6AMiyczdH3f3+xuEPgB8ysz2URvcXFcXO4ZaeWQvtdr5j6mVVeqPexD4TWAW8HV14i9nWtBBRCRO+msmIhIpdeAiIpFSBy4iEil14CIikSr0Qp4JNtG7mdLSY1pn4+mh1endwX0qB1JmzD1/IByTUtvH7mfcfWbR5z1qRsUXzOsq+rSFeXSt7k3VbqHcLrQD72YKZ7X4hnid03sbbu9fGp6RN2P94VNV69z1s9E2SdrkNr9+8/A/1XoL5nVx7y3z23HqQrxtzmntbsK4F8ptlVBERCKlDlxEJFLR38yquqvxXVD3HRfeZ8ZVKpOI1FOZJE4agYuIREoduIhIpKIooVhl5HeSPPayn2Y6ng8MjPhcIrG7ZVu4rKjySnlpBC4iEil14CIikVIHLiISqaZq4Ml6dV+jtq6dU1uY4OfAd4EFwBPAxe6+O3NDehtfUQlA98RgqLp1W8Pt/73tweA+75hzRtPtqrf/PWcHYz3X3p3pmNJeReR27NLq40VSLf6Vmh2B/zPwQ3c/GTiN2uoZlwKr3X0hsDp5LhIb5bZEa9gO3MymAW8CroLaEkfuvge4CFiV/NgqYGk+TRTJh3JbYtdMCeU44Glqq6qfBqwBPgTMcvftyc/soLZm3SuY2QpgBUA34buaDe7bF44FrrYE2P/bjcsa75gT3AVSltXrmBS+i6HKJGNO5tyuz+v5fVHMxs2Fyhrt1UwJpRM4E/iyu59BbRXzl72l9NrCmg0X13T3le6+2N0XdxGuZYu0Qebcrs/rmb0jv05BpBWa6cC3AFvc/Z7k+fXUkn6nmc0GSB7782miSG6U2xK1Yd/7ufsOM3vKzE5y958D5wPrk69lwBXJ402jacjgwYPhRqbMUJn+kycbbq+mnKsjZVbL4AsvpOwZVunpCcYG9u/PdEzJV1G5PZZlnaGi0ktrNFu8+zPgajObAGwC/oDa6P06M1sObAYuzqeJIrlSbku0murA3f0hYHGDUGuX1xEpmHJbYqYrMUVEIhXH/KdK+O/MwM6nR3y4wQOtX7hYdW4Zq1SvLi+NwEVEIqUOXEQkUlGUUPp+EJ7at/msxgswVKZNC+5jU8JXhFa3bQ/G0mz76OuDsTmfC1/B2TFhQjCWNrXypQsbfe5WM/E/7g3GREaqLDezymosl4A0AhcRiZQ6cBGRSKkDFxGJVHlq4Cl3CDx5SrguvdmPaLh9YG/47oadE8N15zQdE8N3Kpzz2fAiymkGX3ox036qc0vZjeXac1loBC4iEil14CIikSpNCcU6LBi75ZTpKXsONt7sge1AtX/kV29C9nKHSBmopDH2aAQuIhIpdeAiIpEqTQmlMvuYYGxg+460PUd8Lh9ofPUmQOf8uSM+HkD1yS2Z9hMpStFXVKpkkz+NwEVEIqUOXEQkUurARUQiVZoa+GDK1L7tf3ZWMDbnKw82Pl7GRRtUy5axSjXpsUcjcBGRSKkDFxGJVHlKKCmLF4TKJBAulXSedGJwn4PHNL4BFkDHjx8IxkRilnUaoUov5aURuIhIpNSBi4hESh24iEikmqqBm9kTwD5gAKi6+2IzmwF8F1gAPAFc7O67Mzdk4QnBWHXj4yM+XvXRTcHYhBf7wvuN+EwSsyJyWyQvIxmBv9ndT3f3oeXQLwVWu/tCYHXyXCRGym2J0mhKKBcBq5LvVwFLR90akXJQbksUmp1G6MCPzMyBr7j7SmCWuw8tVrkDmNVoRzNbAawA6GZy8ARZyiQAR/xkZsPt3zv+tuA+b5sTXuxBxp1MuV2f1/P7SjMbNzNNFYxTs5n3RnffamZHA7ea2f/VB93dk1+AV0h+IVYCHGEzGv6MSBtlyu36vF58WrfyWtqiqRKKu29NHvuB7wNLgJ1mNhsgeezPq5EieVFuS8yGHYGb2RSgw933Jd+/FfgUcDOwDLgiebxpNA2xSnhhhrQFGPb+6q6G29/m4beEnb29wVh1V+PjQfY2SjkVldsxyGOxB5Vl8tdMCWUW8H0zG/r577j7D83sPuA6M1sObAYuzq+ZIrlQbkvUhu3A3X0T8Io/pe6+Czg/j0aJFEG5LbHTlZgiIpEqzfynrDXkju6JI96nf+lJwdiMq34ajGVtY6WnJxgb2L8/0zFFyi5LXV1185HRCFxEJFLqwEVEIlWaEkqa6m3zg7HOX3uy4fa0skVamSQPKpNI2al0ESeNwEVEIqUOXEQkUurARUQiFUUNfMIFO4Kx0H0F0+rOHa89JXyy9eG7IoYWUAaoTJsWjA0891z4fCIlkMel9GlUc28NjcBFRCKlDlxEJFKlKaF0TJiQab/Bc89suL3r6XAJZWDNw5nOlUZlEomZShpx0ghcRCRS6sBFRCJVmhLK4MGDwVjaQgqHehr/Ezp+/FimdnQevyAYq256ItMxRcou6ywUlV7aSyNwEZFIqQMXEYmUOnARkUiVpgae5j2PbAnGrjm5tYsJq84t45Fq2XHSCFxEJFLqwEVEIhVFCeXa18xNiQZKKBb+25S2jmZHz5RgrPr0MyntEImXphHGSSNwEZFIqQMXEYmUOnARkUg1XQM3swpwP7DV3S80s+OAa4FeYA3wPncPXw8/jE2fe10wdvxH7hrx8SpHTA3G0u4c2HH0USM+l8Qr77wWydNIRuAfAjbUPf8McKW7nwjsBpa3smEiBVFeS7Sa6sDNbC7wTuBryXMDzgOuT35kFbA0h/aJ5EZ5LbFrtoTyBeCvgaG6RC+wx92ryfMtQF+jHc1sBbACoJvJwRMc/9F7mmzKYcfv7Gq4vevmScF9Bs4Jl1Cqm5/K1A6J0hdoQV7P74tiNm4qTQeM07AjcDO7EOh39zVZTuDuK919sbsv7iI8/1qkSK3M65m94dsdi+SpmaHDG4DfMLMLgG7gCOCfgelm1pmMVuYCW/NrpkjLKa8lesN24O7+N8DfAJjZucBH3P0SM/se8G5qn9gvA24aTUMqKVdAkrKgw8BzextuP3TRi5nakbZ4RGXW0eF29Iev0vTqoUxtkfwUldexyHolZquplDMyo5kH/jHgr8zsMWq1w6ta0ySRtlJeSzRG9OmLu98B3JF8vwlY0vomiRRLeS2x0pWYIiKRKs38p4F9+4KxG7aEpxj+1gnnNNxuk7rDJ9sTDvlAeIGI6rbt4R1FIqbac5w0AhcRiZQ6cBGRSJWmhNJ59MxgLFQmAXjp3FMbbvdOC+4z4T93NN8wEZGS0ghcRCRS6sBFRCJVmhJKtf/pYKxzfnhNzK5b7mu4PfMVlc/sCsbSZqh0zj4mGKtuV8lGyq3oKzE166U1NAIXEYmUOnARkUipAxcRiVRpauBpBrbtDAdt5H+Dqjv7g7HK1JS1NFOuFlWdW8Yj1bLbSyNwEZFIqQMXEYlUFCWUtAURQtP3BveE1730F18Kxgb2Px+M7fqj1wdjvV/5aTBWmT493JaXwm2xBfOCsYENjwZjIkXRQhDtpRG4iEik1IGLiERKHbiISKSiqIFXenqCsQ2XH9tw+8I/CS8CkTb1sHLktGDsqKsaX7YP4CnHDC28DPCLK84Kxo772F3BmMh4M17r3Gk0AhcRiZQ6cBGRSEVRQrHJk4KxUKmkY1J4n8EDB4KxgWd3pzQkpfTSMyUY82o1GEsrkzz9gfC0xZlfvjsY6+gK/7cOHjwYjImUWR5TFmMvy2gELiISKXXgIiKRKk0JxX/1jHBs3S9GfLy0MklmPhgMpd3oKquZ/xq+ujONyiTjV+wlARmZYUfgZtZtZvea2c/M7BEz+2Sy/Tgzu8fMHjOz75rZhPybK9I6ym2JXTMllJeA89z9NOB04O1mdjbwGeBKdz8R2A0sz62VIvlQbkvUhu3AvWZ/8rQr+XLgPOD6ZPsqYGkeDRTJi3JbYtdUDdzMKsAa4ETgS8DjwB53H5oftwXoC+y7AlgB0M3k8Dn+98FgLLyUsMjoZM3t+rye31eaj5IyT7VT7TxOTc1CcfcBdz8dmAssAU5u9gTuvtLdF7v74i4mZmulSE6y5nZ9Xs/sreTZRJGgEU0jdPc9wO3A64DpZjY09JgLbG1t00SKo9yWGA373s/MZgKH3H2PmU0C3kLtQ57bgXcD1wLLgJvybGhIaPphx10PZzveQLaCTWVKuDw0sH9/MCbtU/bcLlLRCzOoZNMazRTvZgOrklphB3Cdu//AzNYD15rZ3wMPAlfl2E6RPCi3JWrDduDuvhZ4xTDX3TdRqxmKREm5LbHTpfQiIpEqzfwnq4Q/ybfOrmBsMDD9MG2Bhc75DWc8AlDd/FQwVnn1q4IxLTIs8nKqc+dPI3ARkUipAxcRiVRpSiiPfTb8mdEJfxVe9CBUXvHqoeA+vitl0YYUKpPIeKRSSHlpBC4iEil14CIikSpNCeWEDzde2xJg1x+F14bs/UrjRQ86JoevjLzw3s3B2A9ef0IwNrBnTzAmMlalXaWp8kp7aQQuIhIpdeAiIpFSBy4iEqnS1MDTFgwO1bmz+sFZC4KxwQPPh3dMubrTlpwSjPk9a5tplkh0tIBEe2kELiISKXXgIiKRKk8JJaOOid0Ntw++8EKm42W9YZXKJCIvpzJJ/jQCFxGJlDpwEZFIqQMXEYlUaWrgoVo2wOBLL4489j9zwyc7b0swpDsOynikenWcNAIXEYmUOnARkUiVpoSSViap9PQEY37wYOPtb90Z3Kdj+vRgTHcclPEo6xWVaVSWyZ9G4CIikVIHLiISqdKUUE55oBKMPfza8FWVHd0TG273gweC+6hMIuORShpjz7AjcDObZ2a3m9l6M3vEzD6UbJ9hZrea2cbk8cj8myvSOsptiV0zJZQq8GF3XwScDXzQzBYBlwKr3X0hsDp5LhIT5bZEbdgO3N23u/sDyff7gA1AH3ARsCr5sVXA0pzaKJIL5bbEbkQ1cDNbAJwB3APMcvftSWgHMCuwzwpgBUA34YWGHz5zYCRN+SU/VA00NuVvU8riEblIa0uatHamHPOpy84OxuZ9OmVxjDK9ZgUbaW7X5/X8vtJ8lJQqj6mC8kpFftbQdM9iZj3ADcBfuPve+pi7O+CN9nP3le6+2N0Xd9H4A0eRdsqS2/V5PbM3/AG8SJ6a6sDNrItagl/t7jcmm3ea2ewkPhvoz6eJIvlRbkvMhn3vZ2YGXAVscPfP14VuBpYBVySPN42qJSlv39+9fkcwdv2iYxoHMpYfcikVFHzM1DJJxmOORYXl9himqYnt1Uzx7g3A+4B1ZvZQsu1vqSX3dWa2HNgMXJxLC0Xyo9yWqA3bgbv7TwALhM9vbXNEiqPcltjpUnoRkUiVZv6TVcKf5N9wyuxg7L+33t9w+zvmnBE+2Tir9YqA6tVjkUbgIiKRUgcuIhKp0pRQvHooGEu7U2FqqSTAOrsytSOzjFdibv50+IrKYy/LOFVQxq2xfCXmeC0PaQQuIhIpdeAiIpEqTQklzfo3TAjGOiY13j54ILygQy5lkjQZZ72oTCKtNF7LDGOZRuAiIpFSBy4iEil14CIikSpPDTxlql1aPbvV59p246uDsTnveqTl50tV8BWj277/mmBs8eynGu9z9t6G24FxvUBEGY3laYRjXSVwMbpG4CIikVIHLiISqfKUUFJ09s0Jxqpbt7X0XJnLJGkiKRek/dszvcqR/LslO01NLMrGhls1AhcRiZQ6cBGRSKkDFxGJVHlq4Cn10rQ6d8eZjae+DT6QUstWbVakaapzl5dG4CIikVIHLiISqdKUUJ77vdcFY9O+dU8wdmhad8Pt4SUgYPf7w+c6MDO0SDnM+Uz47oCds48Jxjb8w9xgbOH7G6/pKVIWRV7BqXLNyGgELiISKXXgIiKRKk0JpffO8EyTasqskUNTGxdL0kooM76ZrWzhKbHqjv5gbOqMGZnOJyKSZtgRuJl93cz6zezhum0zzOxWM9uYPB6ZbzNFWk+5LbFrpoTyDeDth227FFjt7guB1clzkdh8A+W2RGzYDtzd7wSePWzzRcCq5PtVwNLWNkskf8ptiV3WGvgsd9+efL8DmBX6QTNbAawA6GZy8IDPnBO+4+D0JzYHY5N/+FDD7WnXWuayqHFKnf6Ypetbfz7JS1O5XZ/X8/tK81FSKk3RG3tGPQvF3Z2Uz/fcfaW7L3b3xV1MHO3pRAqTltv1eT2zN+0jc5H8ZO3Ad5rZbIDkMTwFQyQuym2JRtb3fjcDy4ArksebRtuQI68OT+2zSZOCsQ+sW9tw+xdPXDjaJsn41PLcFslLM9MIrwHuAk4ysy1mtpxacr/FzDYCv5Y8F4mKcltiN+wI3N3fGwid3+K2iBRKuS2x06X0IiKRKs38J+sKN8UmNb7jIGSrdXectigY23Tx9GDsxC+HpzNWt24PxqwSnqWQy5RGkQaKvKtgVprqODIagYuIREoduIhIpEpTQhk8cCAY2/W9+cFY74W7Gwcs/LfJHn8yGFvw8fBVk9VgJJ1XtQanxEtljfLSCFxEJFLqwEVEIlWaEkqa3gt/HoxZZ1fD7ZVpRwT3GdizJ3yylNLLC7+5JBibfMPd4WOKRCzr7BWVXvKnEbiISKTUgYuIREoduIhIpKKogafVpX1goOH2N96xJbjPHaeG726YRnVuESkTjcBFRCKlDlxEJFKlKaGEpgMCDL7ulPB+P2k8xenOZYtTzvZIMFKZcWQwNvBs4KpPkXFKUwXbSyNwEZFIqQMXEYmUOnARkUiVpgaetrBBZU34UvrQff4GHwjXudOozi3jkWrZcdIIXEQkUurARUQiVZoSSsfE8LqX1hH+O/ORx9Y13P65E14T3KcydWow5tXwsg1pi05UXv2qYGxgw6PBWJpKT0/4mPv3ZzqmSCMxrJeZh9hLRxqBi4hESh24iEikSlNCCd2UCuDRry0KxnYNhNewDBnYty8Y6zx2XjA2uPmp8DEzlknSqEwiZRd7CSJ2oxqBm9nbzeznZvaYmV3aqkaJtJtyW2KQuQM3swrwJeAdwCLgvWYWHiqLREK5LbEYzQh8CfCYu29y94PAtcBFrWmWSFsptyUKo6mB9wH1ReEtwFmH/5CZrQBWJE9fus2vf7jh0cIXYsLvXhMMXRKM3JtyQI4CnmkYeSJtt1yE21K8srQlazuObdH5h83tw/O6Mntj47wuXsH/hxvTgrHnUx5amtu5f4jp7iuBlQBmdr+7p93ntRBlaQeoLWVuR5oy5jWoLWVuB7S+LaMpoWwF6qdszE22icROuS1RGE0Hfh+w0MyOM7MJwHuAm1vTLJG2Um5LFDKXUNy9amZ/CtwCVICvu/twtwBcmfV8LVaWdoDa0khb25Eht8vyuoHa0khZ2gEtbou5eyuPJyIiBdGl9CIikVIHLiISqUI68DJdlmxmT5jZOjN7yMzuL/jcXzezfjN7uG7bDDO71cw2Jo9HtqkdnzCzrcnr8pCZXZB3O5LzzjOz281svZk9YmYfSrYX/rpkodwuT16ntKXw3C4qr3PvwEt6WfKb3f30NswN/Qbw9sO2XQqsdveFwOrkeTvaAXBl8rqc7u7/VUA7AKrAh919EXA28MEkP9rxuoyIcvuXvkE58jrUFig+twvJ6yJG4LosOeHudwLPHrb5ImBV8v0qYGmb2tEW7r7d3R9Ivt8HbKB2JWThr0sGym3Kk9cpbSlcUXldRAfe6LLkvgLOG+LAj8xsTXI5dLvNcvftyfc7gFltbMufmtna5G1o4SULM1sAnAHcQ7lelxDldljZ/v/altt55vV4/BDzje5+JrW3vR80sze1u0FDvDans13zOr8MnACcDmwH/qnIk5tZD3AD8Bfuvrc+1ubXJSalzO0S/P+1LbfzzusiOvBSXZbs7luTx37g+9TeBrfTTjObDZA89rejEe6+090H3H0Q+CoFvi5m1kUtya929xuTzaV4XYah3A4rzf9fu3K7iLwuogMvzWXJZjbFzKYOfQ+8FWj3XeRuBpYl3y8DbmpHI4aSKvEuCnpdzMyAq4AN7v75ulApXpdhKLfDSvP/147cLiyv3T33L+AC4FHgceDjRZwz0I7jgZ8lX48U3RbgGmpv4Q5Rq5cuB3qpfRq9EbgNmNGmdnwLWAesTZJsdkGvyRupvY1cCzyUfF3QjtclY/vHfW6XJa9T2lJ4bheV17qUXkQkUuPxQ0wRkTFBHbiISKTUgYuIREoduIhIpNSBi4hESh24iEik1IGLiETq/wH4Trx4DKL+QwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a map and  a mask.\n",
    "# y axis represents the different strings,\n",
    "# x axis represents the word in the strings\n",
    "# the activation of a cell represents the magnitude of the token\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title(\"Token IDs\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(example_tokens!=0)\n",
    "plt.title(\"Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder/decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding1 = tf.keras.layers.Embedding(\n",
    "            self.input_vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        # The GRU layer proceses these embedding vectors as sequences\n",
    "        self.gru1 = tf.keras.layers.GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        vectors = self.embedding1(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # The GRU processes the embedding sequence\n",
    "        # Output Shape: (batch, s, enc_units)\n",
    "        # State shape: (batch, enc_units)\n",
    "        output, state = self.gru1(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape (batch): (64,)\n",
      "Input batch tokens shape (batch, s): (64, 22)\n",
      "Encoder output shape (batch, s, units): (64, 22, 1024)\n",
      "Encoder state shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = Encoder(\n",
    "    input_text_processor.vocabulary_size(),\n",
    "    embedding_dim,\n",
    "    units\n",
    ")\n",
    "\n",
    "# Get encoder output and state by passing the example through the encoder\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attention_units'))\n",
    "\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attention_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "\n",
    "        shape_checker(context_vector, ('batch', 's', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer_test = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 22])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units):           (64, 22, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 22, 22)\n"
     ]
    }
   ],
   "source": [
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 22, 10])\n",
    "\n",
    "context_vector, attention_weights = attention_layer_test(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0)\n",
    ")\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THe attention weights should sum to 1 for each sequence\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title(\"Attention weights\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(example_tokens!=0)\n",
    "plt.title(\"Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 22, 22])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding1 = tf.keras.layers.Embedding(\n",
    "            self.output_vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        self.gru1 = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "        self.attention1 = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        self.Wc = tf.keras.layers.Dense(\n",
    "            dec_units,\n",
    "            activation=tf.math.tanh,\n",
    "            use_bias=False\n",
    "        )\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "\n",
    "    class DecoderInput(typing.NamedTuple):\n",
    "        new_tokens: Any\n",
    "        enc_output: Any\n",
    "        mask: Any\n",
    "    \n",
    "    class DecoderOutput(typing.NamedTuple):\n",
    "        logits: Any\n",
    "        attention_weights: Any\n",
    "\n",
    "    def call(self,\n",
    "            inputs: DecoderInput,\n",
    "            state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "        if state is not None:\n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Lookup the embeddings\n",
    "        vectors = self.embedding1(inputs.new_tokens)\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "        # Process one step with the RNN\n",
    "        rnn_output, state = self.gru1(vectors, initial_state=state)\n",
    "\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Use the RNN output as the query for the attention over the encoder\n",
    "        # output\n",
    "\n",
    "        context_vector, attention_weights = self.attention1(\n",
    "            query=rnn_output,\n",
    "            value=inputs.enc_output,\n",
    "            mask=inputs.mask\n",
    "        )\n",
    "        \n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "        \n",
    "        # Join the context vector and rnn_output\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "        \n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "        logits = self.fc(attention_vector)\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    output_text_processor.vocabulary_size(),\n",
    "    embedding_dim,\n",
    "    units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('seq2seq-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b1576f7c6a75cfe22f197e87d4cc49d76ff0c6690cc3efd57128c1cf81b92ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
